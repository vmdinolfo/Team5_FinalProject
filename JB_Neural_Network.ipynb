{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do you know your stroke risk?\n",
    "\n",
    "Source of the data: https://www.sciencedirect.com/science/article/pii/S0933365719302295?via%3Dihub\n",
    "Liu, Tianyu; Fan, Wenhui; Wu, Cheng (2019), “Data for: A hybrid machine learning approach to cerebral stroke prediction based on imbalanced medical-datasets”, Mendeley Data, V1, doi: 10.17632/x8ygrw87jw.1\n",
    "\n",
    "The medical dataset contains 43,400 records of potential patients which includes 783 occurrences of stroke. \n",
    "\n",
    "Cerebral stroke has become a significant global public health issue. The ideal solution to this concern is to prevent in advance by controlling related metabolic factors. However, it is difficult for medical staff to decide whether special precautions are needed for a potential patient only based on the monitoring of physiological indicators unless they are obviously abnormal. This project builds a machine learning model to predict whether someone is at risk of having a stroke.\n",
    "\n",
    "The data in each row includes numerical factors, such as age and average glucose levels, and categorical factors, such as \"has heart disease\" (yes or no), work type, and smoking status. This is not an exhaustive list. We use this data to determine which factors contribute to having a stroke, and among those which hold the most weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic analysis of the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies and Setup\n",
    "import pandas as pd\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as sts\n",
    "#import seaborn as sns\n",
    "%matplotlib inline\n",
    "#sns.set_style('whitegrid')\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#tensorflow.keras.__version__\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import imblearn\n",
    "print(imblearn.__version__)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Read the golf course dataset\n",
    "input_csv = pd.read_csv(\"data/stroke_dataset.csv\", delimiter=',', skipinitialspace=True)\n",
    "input_csv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Code Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(X,y, filename):\n",
    "    dim = X.shape[1]\n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"**********************\")\n",
    "    print(f\"Before X: {X.shape}\")\n",
    "    print(f\"Before y: {y.shape}\")\n",
    "    \n",
    "    sm = SMOTE()\n",
    "    X, y = sm.fit_resample(X,y)\n",
    "      \n",
    "    print(\"**********************\")\n",
    "    print(f\"SMOTED X: {X.shape}\")\n",
    "    print(f\"SMOTED y: {y.shape}\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 50)\n",
    "    \n",
    "    X_scaler = MinMaxScaler().fit(X_train)\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "    # Step 1: Label-encode data set\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(y_train)\n",
    "    encoded_y_train = label_encoder.transform(y_train)\n",
    "    encoded_y_test = label_encoder.transform(y_test)\n",
    "    # Step 2: Convert encoded labels to one-hot-encoding\n",
    "    y_train_categorical = to_categorical(encoded_y_train)\n",
    "    y_test_categorical = to_categorical(encoded_y_test)\n",
    "    \n",
    "    print(f\"X_scaler: {X_scaler}\")\n",
    "    unit_num = 150\n",
    "    \n",
    "    # Create model and add layers\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=unit_num, activation='relu', input_dim=dim))\n",
    "    model.add(Dense(units=unit_num, activation='relu'))\n",
    "    model.add(Dense(units=unit_num, activation='relu'))\n",
    "    model.add(Dense(units=unit_num, activation='relu'))\n",
    "    model.add(Dense(units=unit_num, activation='relu'))\n",
    "    \n",
    "    \n",
    "#     model.add(Dense(units=unit_num, activation='relu'))    \n",
    "#     model.add(Dense(units=unit_num), activation='relu'))\n",
    "    model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "    #categorical_crossentropy\n",
    "    \n",
    "    # Compile and fit the model\n",
    "    model.compile(optimizer='Adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=[tf.keras.metrics.PrecisionAtRecall(recall = 0.85)])\n",
    "    \n",
    "    model.summary()\n",
    "    model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=100,\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    "    )\n",
    "    model_result = model.evaluate(\n",
    "        X_test_scaled, y_test_categorical, verbose=2)\n",
    "    \n",
    "    print(\"/n\")\n",
    "    print(\"Test Results\")\n",
    "    print(f\"Normal Neural Network - Loss: {model_result[0]}, Accuracy: {model_result[1]}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    uniqueValues_train, occurCount_train = np.unique(y_train, return_counts=True)\n",
    "    uniqueValues_test, occurCount_test = np.unique(y_test, return_counts=True)\n",
    "    print(f\"train unique values {uniqueValues_train}\")\n",
    "    print(f\"train occur count {occurCount_train}\")\n",
    "    \n",
    "    print(f\"test unique values {uniqueValues_test}\")\n",
    "    print(f\"test occur count {occurCount_test}\")\n",
    "   \n",
    "    model.save(filename+\".h5\")\n",
    "    \n",
    "    return (model_result[1], model_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the input data for preview\n",
    "input_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting histogram of age\n",
    "\n",
    "x = input_csv['age']\n",
    "num_bins = 20\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(x, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.ylim([1000, 3000])\n",
    "# add a 'best fit' line\n",
    "#y = mlab.normpdf(bins, mu, sigma)\n",
    "\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Probability')\n",
    "plt.title(r'Histogram of Age')\n",
    "\n",
    "# Tweak spacing to prevent clipping of ylabel\n",
    "plt.subplots_adjust(left=0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking into balanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_data_df = input_csv.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_data_df = stroke_data_df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolating Positive Stroke Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_positive = stroke_data_df[stroke_data_df['stroke'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "stroke_positive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_positive.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolating Negative Stroke Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_negative = stroke_data_df[stroke_data_df['stroke'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_negative.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_negative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Number of Stroke Positive {len(stroke_positive)}\")\n",
    "print(f\"Total Number of Stroke Negative {len(stroke_negative)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping String Data to Numeric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gender = {\"Other\":2,\"Male\":1, \"Female\":0}\n",
    "Married = {\"Yes\":1, \"No\":0}\n",
    "Work_Type = {\"Private\":0, \"Self-employed\":1, \"children\":2, \n",
    "             \"Govt_job\":3,\"Never_worked\":4}\n",
    "Residence = {\"Urban\":0, \"Rural\":1}\n",
    "Smoking = {\"never smoked\":0, \"formerly smoked\":1, \"smokes\":2, \"unknown\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stroke_negative[\"smoking_status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_negative = stroke_negative.replace({\"gender\":Gender, \"ever_married\":Married,\n",
    "                                          \"work_type\":Work_Type, \"Residence_type\":Residence,\n",
    "                                          \"smoking_status\":Smoking})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_positive = stroke_positive.replace({\"gender\":Gender, \"ever_married\":Married,\n",
    "                                          \"work_type\":Work_Type, \"Residence_type\":Residence,\n",
    "                                          \"smoking_status\":Smoking})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_negative.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_negative.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating various Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data Set for Analysis - Equal set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "column_headings=[\"Loss\",\"Accuracy\",\"TP\", \"TN\", \"FP\", \"FN\"]\n",
    "results_df = pd.DataFrame(columns = column_headings)\n",
    "for i in range(0,1):\n",
    "    \n",
    "    stroke_negative_sample = stroke_negative.sample(548)\n",
    "    stroke_negative_sample = stroke_negative_sample.drop(\"id\",axis = 1)\n",
    "    stroke_positive_sample = stroke_positive.sample(28524)\n",
    "    stroke_positive_sample = stroke_positive_sample.drop(\"id\",axis = 1)\n",
    "    \n",
    "    stroke_sample = pd.merge(stroke_negative_sample, stroke_positive_sample, how = 'outer')\n",
    "\n",
    "    print(f\"Negative data set {len(stroke_negative_sample)}\")\n",
    "    print(f\"Positive data set {len(stroke_positive_sample)}\")\n",
    "    print(f\"Combined data set {len(stroke_sample)}\")\n",
    "    print(f\"Shape of combined {stroke_sample.shape}\")\n",
    "\n",
    "    X = stroke_sample.drop(\"stroke\", axis = 1)\n",
    "    y = stroke_sample[\"stroke\"]\n",
    "    filename = \"junk-1\"\n",
    "\n",
    "    model_perf = neural_net(X,y, filename)\n",
    "\n",
    "    ## Loading a model to test performance\n",
    "\n",
    "    \n",
    "    # Load the model\n",
    "    from tensorflow.keras.models import load_model\n",
    "    #filename = \"NN_1B\"\n",
    "    stroke_model = load_model(filename+\".h5\")\n",
    "\n",
    "    stroke_negative_sample = stroke_negative.sample(548)\n",
    "    stroke_negative_sample = stroke_negative_sample.drop(\"id\", axis = 1)\n",
    "    stroke_positive_sample = stroke_positive.sample(28524)\n",
    "    stroke_positive_sample = stroke_positive_sample.drop(\"id\",axis = 1)\n",
    "    stroke_sample = pd.merge(stroke_negative_sample, stroke_positive_sample, how = 'outer')\n",
    "\n",
    "    X = stroke_sample.drop(\"stroke\", axis = 1)\n",
    "    y = stroke_sample[\"stroke\"]\n",
    "    print(X.shape, y.shape)\n",
    "\n",
    "    X_scaler = MinMaxScaler().fit(X)\n",
    "    X_scaled = X_scaler.transform(X)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(y)\n",
    "    encoded_y = label_encoder.transform(y)\n",
    "    y_categorical = to_categorical(encoded_y)\n",
    "\n",
    "    encoded_predictions = stroke_model.predict_classes(X_scaled)\n",
    "\n",
    "    TN = 0\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    for i in range(0,len(encoded_y)):\n",
    "        if encoded_predictions[i] == 0:\n",
    "            if encoded_y[i] == 0:\n",
    "                TN += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "        if encoded_predictions[i] == 1:\n",
    "            if encoded_y[i] == 1:\n",
    "                TP += 1\n",
    "            else: \n",
    "                FP += 1\n",
    "    total = FP+FN+TN+TP\n",
    "    print(f\"Total samples: {total}\")\n",
    "    print(f\"True Positive: {TP}\")\n",
    "    print(f\"True Negative: {TN}\")\n",
    "    print(f\"False Positive: {FP}\")\n",
    "    print(f\"False Negative: {FN}\")\n",
    "    \n",
    "    result = {\"Loss\":model_perf[0], \"Accuracy\":model_perf[1], \"TP\":TP, \"TN\":TN, \"FP\":FP, \"FN\":FN}\n",
    "    results_df = results_df.append(result, ignore_index = True)\n",
    "    \n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(filename+\".csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data set of values for different training sets and layer numbers was setup for analysis.  Each was run independently and then exported as a CSV file.  These were imported into excel for basic combining.  A master CSV was generated to allow comparison of conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "nn_perf_df = pd.read_csv(\"support_docs\\master_data_NN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_perf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_perf_df[\"Layers\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_perf_df[\"Accuracy\"] = (nn_perf_df[\"Accuracy\"].str.strip(\"%\").astype(float))\n",
    "nn_perf_df[\"Precision\"] = (nn_perf_df[\"Precision\"].str.strip(\"%\").astype(float))\n",
    "nn_perf_df[\"Recall\"] = (nn_perf_df[\"Recall\"].str.strip(\"%\").astype(float))\n",
    "nn_perf_df[\"F1\"] = (nn_perf_df[\"F1\"].str.strip(\"%\").astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_perf_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn_perf_df[\"Layer 1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of Layers and Training Size\n",
    "\n",
    "from matplotlib import cm\n",
    "layers = [1,4]\n",
    "training = [548, 20000]\n",
    "\n",
    "#plot 1\n",
    "red_diamond = dict(markerfacecolor='r', marker='D')\n",
    "fig = plt.figure(figsize = (15,12))\n",
    "ax1 = fig.add_subplot(321)\n",
    "#fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('Accuracy vs Number of Layers')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim(-5,100)\n",
    "ax1.boxplot([nn_perf_df[\"Accuracy\"][nn_perf_df[\"Layers\"]==layers[i]] for i in range(0,len(layers))], labels=layers, showfliers = True,\n",
    "           flierprops = red_diamond)\n",
    "\n",
    "#plot 2\n",
    "ax2 = fig.add_subplot(322)\n",
    "#fig1, ax1 = plt.subplots()\n",
    "ax2.set_title('Recall vs Number of Layers')\n",
    "ax2.set_ylabel('Recall')\n",
    "ax2.set_ylim(-5,100)\n",
    "\n",
    "x=[]\n",
    "y =[]\n",
    "for i in range(0,len(nn_perf_df)):\n",
    "    if nn_perf_df.iloc[i][3] == 4:\n",
    "        x.append(1.75)\n",
    "    else:\n",
    "        x.append(0.75)\n",
    "    y.append(nn_perf_df.iloc[i][27])\n",
    "ax2.scatter(x,y, c=cm.prism(x), alpha = 0.4)\n",
    "ax2.boxplot([nn_perf_df[\"Recall\"][nn_perf_df[\"Layers\"]==layers[i]] for i in range(0,len(layers))], labels=layers, showfliers = True,\n",
    "          flierprops = red_diamond)\n",
    "\n",
    "#plot 3\n",
    "ax3 = fig.add_subplot(323)\n",
    "#fig1, ax1 = plt.subplots()\n",
    "ax3.set_title('Accuracy vs Training Size')\n",
    "ax3.set_ylabel('Accuracy')\n",
    "ax3.set_ylim(-5,100)\n",
    "ax3.boxplot([nn_perf_df[\"Accuracy\"][nn_perf_df[\"N-Negative\"]==training[i]] for i in range(0,len(training))], labels=training, showfliers = True,\n",
    "           flierprops = red_diamond)\n",
    "\n",
    "#plot4\n",
    "ax4 = fig.add_subplot(324)\n",
    "#fig1, ax1 = plt.subplots()\n",
    "ax4.set_title('Recall vs Training Size')\n",
    "ax4.set_ylabel('Recall')\n",
    "ax4.set_ylim(-5,100)\n",
    "ax4.boxplot([nn_perf_df[\"Recall\"][nn_perf_df[\"N-Negative\"]==training[i]] for i in range(0,len(training))], labels=training, showfliers = True,\n",
    "           flierprops = red_diamond)\n",
    "\n",
    "#plot5\n",
    "x=[]\n",
    "y =[]\n",
    "for i in range(0,len(nn_perf_df)):\n",
    "    if nn_perf_df.iloc[i][0] == 20000:\n",
    "        x.append(1.75)\n",
    "    else:\n",
    "        x.append(0.75)\n",
    "    y.append(nn_perf_df.iloc[i][26])\n",
    "    \n",
    "ax4 = fig.add_subplot(325)\n",
    "#fig1, ax1 = plt.subplots()\n",
    "ax4.set_title('Precision vs Training Size')\n",
    "ax4.set_ylabel('Precision')\n",
    "ax4.set_ylim(-5,100)\n",
    "ax4.boxplot([nn_perf_df[\"Precision\"][nn_perf_df[\"N-Negative\"]==training[i]] for i in range(0,len(training))], labels=training, showfliers = True,\n",
    "           flierprops = red_diamond)\n",
    "ax4.scatter(x,y, c=cm.prism(x), alpha = 0.4)\n",
    "\n",
    "#plot6\n",
    "ax4 = fig.add_subplot(326)\n",
    "#fig1, ax1 = plt.subplots()\n",
    "ax4.set_title('F1 vs Training Size')\n",
    "ax4.set_ylabel('F1')\n",
    "ax4.set_ylim(-5,100)\n",
    "ax4.boxplot([nn_perf_df[\"F1\"][nn_perf_df[\"N-Negative\"]==training[i]] for i in range(0,len(training))], labels=training, showfliers = True,\n",
    "           flierprops = red_diamond)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(layers[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
