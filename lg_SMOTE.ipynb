{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do you know your stroke risk?\n",
    "\n",
    "Source of the data: https://www.sciencedirect.com/science/article/pii/S0933365719302295?via%3Dihub\n",
    "Liu, Tianyu; Fan, Wenhui; Wu, Cheng (2019), “Data for: A hybrid machine learning approach to cerebral stroke prediction based on imbalanced medical-datasets”, Mendeley Data, V1, doi: 10.17632/x8ygrw87jw.1\n",
    "\n",
    "The medical dataset contains 43,400 records of potential patients which includes 783 occurrences of stroke. \n",
    "\n",
    "Cerebral stroke has become a significant global public health issue. The ideal solution to this concern is to prevent in advance by controlling related metabolic factors. However, it is difficult for medical staff to decide whether special precautions are needed for a potential patient only based on the monitoring of physiological indicators unless they are obviously abnormal. This project builds a machine learning model to predict whether someone is at risk of having a stroke.\n",
    "\n",
    "The data in each row includes numerical factors, such as age and average glucose levels, and categorical factors, such as \"has heart disease\" (yes or no), work type, and smoking status. This is not an exhaustive list. We use this data to determine which factors contribute to having a stroke, and among those which hold the most weight.\n",
    "\n",
    "In this notebook, we build our Machine Learning model. In our initial data analysis, we noticed that the individuals who had a stroke make up approximately 1.8% of the data. We will use the Synthetic Minority Oversampling Technique (SMOTE) to account for this.\n",
    "\n",
    "To view our initial data analysis, please see the notebook titled \"stroke_data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('data/stroke_ML_dataset.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values for each column of the input dataset\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "print(imblearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nulls from df\n",
    "stroke_data_df = data.dropna()\n",
    "\n",
    "# set y as the 'stroke' output, with targets of 0 (No) and 1 (Yes)\n",
    "y = stroke_data_df['stroke']\n",
    "target_names = ['0', '1']\n",
    "\n",
    "# set X as the df after dropping stroke output\n",
    "X = stroke_data_df.drop('stroke', axis=1)\n",
    "\n",
    "# set X as the df after dropping stroke output and id\n",
    "# X = stroke_data_df.drop('stroke', axis=1).drop('id',axis=1)\n",
    "\n",
    "# define a smote instance with default parameters\n",
    "oversample = SMOTE()\n",
    "\n",
    "# rebalance data by applying SMOTE to add instances of 'Yes'\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "# show new counts of output variables by type (should be same)\n",
    "counter = Counter(y)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# over = SMOTE(sampling_strategy=0.1)\n",
    "# under = RandomUnderSampler(sampling_strategy=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_data_df = data.dropna()\n",
    "stroke_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate df for records that have stroke and have no stroke\n",
    "stroke_positive = stroke_data_df[stroke_data_df['stroke'] == 1]\n",
    "stroke_positive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_negative = stroke_data_df[stroke_data_df['stroke'] == 0]\n",
    "stroke_negative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate df for records that have stroke and have no stroke\n",
    "stroke_positive = stroke_data_df[stroke_data_df['stroke'] == 1]\n",
    "stroke_negative = stroke_data_df[stroke_data_df['stroke'] == 0]\n",
    "\n",
    "# return random sample of 500 for both postive and negative results\n",
    "stroke_negative_sample = stroke_negative.sample(750)\n",
    "stroke_positive_sample = stroke_positive.sample(750)\n",
    "\n",
    "# merge postive and negative df to make one combined df\n",
    "stroke_sample = pd.merge(stroke_negative_sample, stroke_positive_sample, how = 'outer')\n",
    "\n",
    "stroke_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Maching Learning algorithm LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Import other essential Machine Learning functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test})\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Data Score: {classifier.score(X_train, y_train)}\")\n",
    "print(f\"Testing Data Score: {classifier.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions,\n",
    "                            target_names=[\"No Stroke\", \"Stroke\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now predict values for the testing data.\n",
    "predictions = logmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test})\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Data Score: {logmodel.score(X_train, y_train)}\")\n",
    "print(f\"Testing Data Score: {logmodel.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use cell below to make predictions with Logistic Regression model\n",
    "\n",
    "### List of input values in order (with codification)\n",
    " - Gender (Female=0,Male=1,Other=2)\n",
    " - Age (actual value)\n",
    " - Hypertension (No=0,Yes=1)\n",
    " - Heart Diserase (No=0,Yes=1)\n",
    " - Married (No=0,Yes=1)\n",
    " - Work Type (Private=0,Self-employed=1,children=2,Govt_job=3,Never_worked=4\")\n",
    " - Residence Type (Urban=0,Rural=1)\n",
    " - Blood Glucose Level (actual value)\n",
    " - BMI (actual value)\n",
    " - Smoking (never smoked=0,formerly smoked=1,smokes=2,unkown=3)\n",
    "\n",
    "### Output prediction value\n",
    " - Are you at risk of having a stroke? (No=0,Yes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [[1,76,1,1,0,0,0,150,32,1]]\n",
    "prediction = logmodel.predict(sample)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [[1,76,1,1,0,0,0,150,32,1]]\n",
    "prediction = classifier.predict(sample)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
