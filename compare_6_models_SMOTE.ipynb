{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do you know your stroke risk?\n",
    "\n",
    "Source of the data: https://www.sciencedirect.com/science/article/pii/S0933365719302295?via%3Dihub\n",
    "Liu, Tianyu; Fan, Wenhui; Wu, Cheng (2019), “Data for: A hybrid machine learning approach to cerebral stroke prediction based on imbalanced medical-datasets”, Mendeley Data, V1, doi: 10.17632/x8ygrw87jw.1\n",
    "\n",
    "The medical dataset contains 43,400 records of potential patients which includes 783 occurrences of stroke. \n",
    "\n",
    "Cerebral stroke has become a significant global public health issue. The ideal solution to this concern is to prevent in advance by controlling related metabolic factors. However, it is difficult for medical staff to decide whether special precautions are needed for a potential patient only based on the monitoring of physiological indicators unless they are obviously abnormal. This project builds a machine learning model to predict whether someone is at risk of having a stroke.\n",
    "\n",
    "The data in each row includes numerical factors, such as age and average glucose levels, and categorical factors, such as \"has heart disease\" (yes or no), work type, and smoking status. This is not an exhaustive list. We use this data to determine which factors contribute to having a stroke, and among those which hold the most weight.\n",
    "\n",
    "In this notebook, we build our Machine Learning model. In our initial data analysis, we noticed that the individuals who had a stroke make up approximately 1.8% of the data. We will use the Synthetic Minority Oversampling Technique (SMOTE) to account for this.\n",
    "\n",
    "To view our initial data analysis, please see the notebook titled \"stroke_data.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Imbalanced Class\n",
    "\n",
    "There are several techniques that can be used to handle highly imbalanced class. This [article](https://heartbeat.fritz.ai/resampling-to-properly-handle-imbalanced-datasets-in-machine-learning-64d82c16ceaa) nicely summarizes those different techniques. In this notebook, we are going to use one of oversampling technique called Synthetic Minority Oversampling Technique (SMOTE), by synthesizing new samples from the minority class to have the same number of samples as the majority class (illustrated in figure below). Over sampling technique is chosen because we do not want to lose significant amount of information (97.88%) as if we use under sampling technique.\n",
    "<figure>\n",
    " <img src=\"https://miro.medium.com/max/1400/1*o_KfyMzF7LITK2DlYm_wHw.png\" style=\"width: 400px;\" alt=\"\"/>\n",
    " <figcaption>\n",
    " Source: <a href=\"https://heartbeat.fritz.ai/resampling-to-properly-handle-imbalanced-datasets-in-machine-learning-64d82c16ceaa\">here</a>\n",
    " </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
    "from sklearn.metrics import auc,roc_auc_score,roc_curve,precision_score,recall_score,f1_score\n",
    "import time as timer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('data/stroke_ML_dataset.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values for each column of the input dataset\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the  object with the desired sampling strategy.\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "\n",
    "# fit the object to our training data\n",
    "X, y = smote.fit_sample(data.loc[:,data.columns!='stroke'], data['stroke'])\n",
    "print(\"Shape of X: {}\".format(X.shape))\n",
    "print(\"Shape of y: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, class_counts = np.unique(y, return_counts=True)\n",
    "class_names = ['No stroke', 'Stroke']\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(class_counts, labels=class_names, autopct='%1.2f%%',\n",
    "        shadow=True, startangle=90, counterclock=False)\n",
    "ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "ax.set_title('Class distribution')\n",
    "plt.show()\n",
    "print(\"# samples associated with no stroke: {}\".format(class_counts[0]))\n",
    "print(\"# samples associated with stroke: {}\".format(class_counts[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_valid_test(X,y,test_size=0.1,random_state=None):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size, random_state=random_state, stratify=y)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train,y_train,test_size=test_size/(1-test_size), random_state=random_state, stratify=y_train)\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test\n",
    "\n",
    "X_train, X_valid, X_test, y_train, y_valid, y_test = split_train_valid_test(X,y,test_size=0.1,random_state=42)\n",
    "_, train_counts = np.unique(y_train, return_counts=True)\n",
    "_, valid_counts = np.unique(y_valid, return_counts=True)\n",
    "_, test_counts = np.unique(y_test, return_counts=True)\n",
    "print(\"[train] # class 0: {} | # class 1: {}\".format(train_counts[0],train_counts[1]))\n",
    "print(\"[valid] # class 0: {} | # class 1: {}\".format(valid_counts[0],valid_counts[1]))\n",
    "print(\"[test]  # class 0: {} | # class 1: {}\".format(test_counts[0],test_counts[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(X_train)\n",
    "\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_valid_std = scaler.transform(X_valid)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ML Model Training and Evaluation\n",
    "We will implement and benchmark the performance of the following 6 ML algorithms:\n",
    "1. Singular Vector Machine (SVM)\n",
    "2. Gaussian Naive Bayes (GNB)\n",
    "3. Logistic Regression (LR)\n",
    "4. Decision Tree (DT)\n",
    "5. Random Forest (RF)\n",
    "6. K Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics\n",
    "\n",
    "The peformance will be evaluated based on two different groups of metrics:\n",
    "1. Sensitivity, specificity, and area under the curve (AUC)\n",
    "2. Precision, recall, and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sens_spec(y_true,y_pred):\n",
    "    conf_matrix = confusion_matrix(y_true,y_pred)\n",
    "    TP = conf_matrix[1][1]\n",
    "    TN = conf_matrix[0][0]\n",
    "    FP = conf_matrix[0][1]\n",
    "    FN = conf_matrix[1][0]\n",
    "    # calculate the sensitivity\n",
    "    sensitivity = TP / (TP + FN)    \n",
    "    # calculate the specificity\n",
    "    specificity = TN / (TN + FP)\n",
    "    return sensitivity,specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer.time()\n",
    "svm_model = SVC(kernel='rbf',probability=True)\n",
    "svm_model.fit(X_train_std, y_train)\n",
    "end = timer.time()\n",
    "print(\"Finished training within {:.2f} seconds\".format(end-start))\n",
    "# Predicting the test set results\n",
    "y_svm = svm_model.predict(X_test_std)\n",
    "y_svm_prob = svm_model.predict_proba(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report for SVM: \\n{}\".format(classification_report(y_test,y_svm)))\n",
    "print(\"Confusion matrix for SVM: \\n{}\".format(confusion_matrix(y_test,y_svm)))\n",
    "print(\"Accuracy score for SVM: {:.2f}\".format(accuracy_score(y_test,y_svm)))\n",
    "# calculate precision, recall, and f1 scores\n",
    "prec_svm = precision_score(y_test,y_svm)\n",
    "rec_svm = recall_score(y_test,y_svm)\n",
    "f1_svm = f1_score(y_test,y_svm)\n",
    "print(\"Precision score for SVM: {:.2f}\".format(prec_svm))\n",
    "print(\"Recall score for SVM: {:.2f}\".format(rec_svm))\n",
    "print(\"F1 score for SVM: {:.2f}\".format(f1_svm))\n",
    "# calculate sensitivity, specificity, and auc\n",
    "sens_svm,spec_svm = calc_sens_spec(y_test,y_svm)\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_svm_prob[:,1])\n",
    "auc_svm = roc_auc_score(y_test, y_svm_prob[:,1])\n",
    "print(\"Sensitivity score for SVM: {:.2f}\".format(sens_svm))\n",
    "print(\"Specitivity score for SVM: {:.2f}\".format(spec_svm))\n",
    "print(\"AUC score for SVM: {:.2f}\".format(auc_svm))\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr, color='blue', label='ROC curve (area = %0.2f)' % auc_svm)\n",
    "ax.plot([0, 1], [0, 1], color='green', linestyle='--')\n",
    "ax.set_xlim([-0.05, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Receiver Operating Characteristic (SVM)')\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes (GNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer.time()\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X_train_std, y_train)\n",
    "end = timer.time()\n",
    "print(\"Finished training within {:.2f} seconds\".format(end-start))\n",
    "# Predicting the test set results\n",
    "y_gnb = gnb_model.predict(X_test_std)\n",
    "y_gnb_prob = gnb_model.predict_proba(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report for GNB: \\n{}\".format(classification_report(y_test,y_gnb)))\n",
    "print(\"Confusion matrix for GNB: \\n{}\".format(confusion_matrix(y_test,y_gnb)))\n",
    "print(\"Accuracy score for GNB: {:.2f}\".format(accuracy_score(y_test,y_gnb)))\n",
    "# calculate precision, recall, and f1 scores\n",
    "prec_gnb = precision_score(y_test,y_gnb)\n",
    "rec_gnb = recall_score(y_test,y_gnb)\n",
    "f1_gnb = f1_score(y_test,y_gnb)\n",
    "print(\"Precision score for GNB: {:.2f}\".format(prec_gnb))\n",
    "print(\"Recall score for GNB: {:.2f}\".format(rec_gnb))\n",
    "print(\"F1 score for GNB: {:.2f}\".format(f1_gnb))\n",
    "# calculate sensitivity, specificity, and auc\n",
    "sens_gnb,spec_gnb = calc_sens_spec(y_test,y_gnb)\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_gnb_prob[:,1])\n",
    "auc_gnb = roc_auc_score(y_test, y_gnb_prob[:,1])\n",
    "print(\"Sensitivity score for GNB: {:.2f}\".format(sens_gnb))\n",
    "print(\"Specitivity score for GNB: {:.2f}\".format(spec_gnb))\n",
    "print(\"AUC score for GNB: {:.2f}\".format(auc_gnb))\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr, color='blue', label='ROC curve (area = %0.2f)' % auc_gnb)\n",
    "ax.plot([0, 1], [0, 1], color='green', linestyle='--')\n",
    "ax.set_xlim([-0.05, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Receiver Operating Characteristic (GNB)')\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer.time()\n",
    "logit_model = LogisticRegression(solver='lbfgs', random_state=42)\n",
    "logit_model.fit(X_train_std, y_train)\n",
    "end = timer.time()\n",
    "print(\"Finished training within {:.2f} seconds\".format(end-start))\n",
    "# Predicting the test set results\n",
    "y_logit = logit_model.predict(X_test_std)\n",
    "y_logit_prob = logit_model.predict_proba(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report for LR: \\n{}\".format(classification_report(y_test,y_logit)))\n",
    "print(\"Confusion matrix for LR: \\n{}\".format(confusion_matrix(y_test,y_logit)))\n",
    "print(\"Accuracy score for LR: {:.2f}\".format(accuracy_score(y_test,y_logit)))\n",
    "# calculate precision, recall, and f1 scores\n",
    "prec_logit = precision_score(y_test,y_logit)\n",
    "rec_logit = recall_score(y_test,y_logit)\n",
    "f1_logit = f1_score(y_test,y_logit)\n",
    "print(\"Precision score for LR: {:.2f}\".format(prec_logit))\n",
    "print(\"Recall score for LR: {:.2f}\".format(rec_logit))\n",
    "print(\"F1 score for LR: {:.2f}\".format(f1_logit))\n",
    "# calculate sensitivity, specificity, and auc\n",
    "sens_logit,spec_logit = calc_sens_spec(y_test,y_logit)\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_logit_prob[:,1])\n",
    "auc_logit = roc_auc_score(y_test, y_logit_prob[:,1])\n",
    "print(\"Sensitivity score for LR: {:.2f}\".format(sens_logit))\n",
    "print(\"Specitivity score for LR: {:.2f}\".format(spec_logit))\n",
    "print(\"AUC score for LR: {:.2f}\".format(auc_logit))\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr, color='blue', label='ROC curve (area = %0.2f)' % auc_logit)\n",
    "ax.plot([0, 1], [0, 1], color='green', linestyle='--')\n",
    "ax.set_xlim([-0.05, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Receiver Operating Characteristic (LR)')\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer.time()\n",
    "dtree_model = DecisionTreeClassifier(random_state=42)\n",
    "dtree_model.fit(X_train_std, y_train)\n",
    "end = timer.time()\n",
    "print(\"Finished training within {:.2f} seconds\".format(end-start))\n",
    "# Predicting the test set results\n",
    "y_dtree = dtree_model.predict(X_test_std)\n",
    "y_dtree_prob = dtree_model.predict_proba(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report for DT: \\n{}\".format(classification_report(y_test,y_dtree)))\n",
    "print(\"Confusion matrix for DT: \\n{}\".format(confusion_matrix(y_test,y_dtree)))\n",
    "print(\"Accuracy score for DT: {:.2f}\".format(accuracy_score(y_test,y_dtree)))\n",
    "# calculate precision, recall, and f1 scores\n",
    "prec_dtree = precision_score(y_test,y_dtree)\n",
    "rec_dtree = recall_score(y_test,y_dtree)\n",
    "f1_dtree = f1_score(y_test,y_dtree)\n",
    "print(\"Precision score for DT: {:.2f}\".format(prec_dtree))\n",
    "print(\"Recall score for DT: {:.2f}\".format(rec_dtree))\n",
    "print(\"F1 score for DT: {:.2f}\".format(f1_dtree))\n",
    "# calculate sensitivity, specificity, and auc\n",
    "sens_dtree,spec_dtree = calc_sens_spec(y_test,y_dtree)\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_dtree_prob[:,1])\n",
    "auc_dtree = roc_auc_score(y_test, y_dtree_prob[:,1])\n",
    "print(\"Sensitivity score for DT: {:.2f}\".format(sens_dtree))\n",
    "print(\"Specitivity score for DT: {:.2f}\".format(spec_dtree))\n",
    "print(\"AUC score for DT: {:.2f}\".format(auc_dtree))\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr, color='blue', label='ROC curve (area = %0.2f)' % auc_dtree)\n",
    "ax.plot([0, 1], [0, 1], color='green', linestyle='--')\n",
    "ax.set_xlim([-0.05, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Receiver Operating Characteristic (DT)')\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer.time()\n",
    "ranfor_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "ranfor_model.fit(X_train_std, y_train)\n",
    "end = timer.time()\n",
    "print(\"Finished training within {:.2f} seconds\".format(end-start))\n",
    "# Predicting the test set results\n",
    "y_ranfor = ranfor_model.predict(X_test_std)\n",
    "y_ranfor_prob = ranfor_model.predict_proba(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report for RF: \\n{}\".format(classification_report(y_test,y_ranfor)))\n",
    "print(\"Confusion matrix for RF: \\n{}\".format(confusion_matrix(y_test,y_ranfor)))\n",
    "print(\"Accuracy score for RF: {:.2f}\".format(accuracy_score(y_test,y_ranfor)))\n",
    "# calculate precision, recall, and f1 scores\n",
    "prec_ranfor = precision_score(y_test,y_ranfor)\n",
    "rec_ranfor = recall_score(y_test,y_ranfor)\n",
    "f1_ranfor = f1_score(y_test,y_ranfor)\n",
    "print(\"Precision score for RF: {:.2f}\".format(prec_ranfor))\n",
    "print(\"Recall score for RF: {:.2f}\".format(rec_ranfor))\n",
    "print(\"F1 score for RF: {:.2f}\".format(f1_ranfor))\n",
    "# calculate sensitivity, specificity, and auc\n",
    "sens_ranfor,spec_ranfor = calc_sens_spec(y_test,y_ranfor)\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_ranfor_prob[:,1])\n",
    "auc_ranfor = roc_auc_score(y_test, y_ranfor_prob[:,1])\n",
    "print(\"Sensitivity score for RF: {:.2f}\".format(sens_ranfor))\n",
    "print(\"Specitivity score for RF: {:.2f}\".format(spec_ranfor))\n",
    "print(\"AUC score for RF: {:.2f}\".format(auc_ranfor))\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr, color='blue', label='ROC curve (area = %0.2f)' % auc_ranfor)\n",
    "ax.plot([0, 1], [0, 1], color='green', linestyle='--')\n",
    "ax.set_xlim([-0.05, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Receiver Operating Characteristic (RF)')\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer.time()\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_std, y_train)\n",
    "end = timer.time()\n",
    "print(\"Finished training within {:.2f} seconds\".format(end-start))\n",
    "# Predicting the test set results\n",
    "y_knn = knn_model.predict(X_test_std)\n",
    "y_knn_prob = knn_model.predict_proba(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report for KNN: \\n{}\".format(classification_report(y_test,y_knn)))\n",
    "print(\"Confusion matrix for KNN: \\n{}\".format(confusion_matrix(y_test,y_knn)))\n",
    "print(\"Accuracy score for KNN: {:.2f}\".format(accuracy_score(y_test,y_knn)))\n",
    "# calculate precision, recall, and f1 scores\n",
    "prec_knn = precision_score(y_test,y_knn)\n",
    "rec_knn = recall_score(y_test,y_knn)\n",
    "f1_knn = f1_score(y_test,y_knn)\n",
    "print(\"Precision score for KNN: {:.2f}\".format(prec_knn))\n",
    "print(\"Recall score for KNN: {:.2f}\".format(rec_knn))\n",
    "print(\"F1 score for KNN: {:.2f}\".format(f1_knn))\n",
    "# calculate sensitivity, specificity, and auc\n",
    "sens_knn,spec_knn = calc_sens_spec(y_test,y_knn)\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_knn_prob[:,1])\n",
    "auc_knn = roc_auc_score(y_test, y_knn_prob[:,1])\n",
    "print(\"Sensitivity score for KNN: {:.2f}\".format(sens_knn))\n",
    "print(\"Specitivity score for KNN: {:.2f}\".format(spec_knn))\n",
    "print(\"AUC score for KNN: {:.2f}\".format(auc_knn))\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr, color='blue', label='ROC curve (area = %0.2f)' % auc_knn)\n",
    "ax.plot([0, 1], [0, 1], color='green', linestyle='--')\n",
    "ax.set_xlim([-0.05, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Receiver Operating Characteristic (DT)')\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmark Across Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_names = ['SVM', 'GNB', 'LR', 'DT', 'RF', 'KNN']\n",
    "sens_all = [sens_svm, sens_gnb, sens_logit, sens_dtree, sens_ranfor, sens_knn]\n",
    "spec_all = [spec_svm, spec_gnb, spec_logit, spec_dtree, spec_ranfor, spec_knn]\n",
    "auc_all = [auc_svm, auc_gnb, auc_logit, auc_dtree, auc_ranfor, auc_knn]\n",
    "\n",
    "prec_all = [prec_svm, prec_gnb, prec_logit, prec_dtree, prec_ranfor, prec_knn]\n",
    "rec_all = [rec_svm, rec_gnb, rec_logit, rec_dtree, rec_ranfor, rec_knn]\n",
    "f1_all = [f1_svm, f1_gnb, f1_logit, f1_dtree, f1_ranfor, f1_knn]\n",
    "\n",
    "def autolabel(bars):\n",
    "    \"\"\"Attach a text label above each bar in displaying its height.\"\"\"\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate('{:.2f}'.format(height),\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 5),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    fontsize=12,\n",
    "                    rotation=90,\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "width = 0.25  # the width of the bars\n",
    "r1 = np.arange(len(ml_names))  # the label locations\n",
    "r2 = [x + width for x in r1]\n",
    "r3 = [x + width for x in r2]\n",
    "# plot sensitivity, specificity, and auc\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "bar1 = ax.bar(r1, sens_all, width, label='Sensitivity')\n",
    "bar2 = ax.bar(r2, spec_all, width, label='Specificity')\n",
    "bar3 = ax.bar(r3, auc_all, width, label='AUC')\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylim([0,1.15])\n",
    "ax.set_ylabel('Scores',fontsize=14)\n",
    "#ax.set_title('Performance benchmark across ML models')\n",
    "ax.set_xticks(r2)\n",
    "ax.set_xticklabels(ml_names)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax.set_xlabel(\"Machine Learning Model\\n(a)\",fontsize=14)\n",
    "ax.legend(loc='lower left',ncol=3,bbox_to_anchor=(0.25,1),fontsize=12)\n",
    "autolabel(bar1)\n",
    "autolabel(bar2)\n",
    "autolabel(bar3)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"support_docs/ML_Benchmark_auc.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# plot sensitivity, specificity, and auc\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "bar1 = ax.bar(r1, prec_all, width, label='Precision')\n",
    "bar2 = ax.bar(r2, rec_all, width, label='Recall')\n",
    "bar3 = ax.bar(r3, f1_all, width, label='F1')\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylim([0,1.13])\n",
    "ax.set_ylabel('Scores',fontsize=14)\n",
    "#ax.set_title('Performance benchmark across ML models')\n",
    "ax.set_xticks(r2)\n",
    "ax.set_xticklabels(ml_names)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax.set_xlabel(\"Machine Learning Model\\n(b)\",fontsize=14)\n",
    "ax.legend(loc='lower left',ncol=3,bbox_to_anchor=(0.25,1),fontsize=12)\n",
    "autolabel(bar1)\n",
    "autolabel(bar2)\n",
    "autolabel(bar3)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"support_docs/ML_Benchmark_f1.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Most Important Features\n",
    "\n",
    "To evaluate the most important features, we will use feature importance score which can be calculated using tree based feature importance. This is calculated during the construction of the boosted decision trees within the model. The more an attribute is used to make key decisions with decision trees indicates higher relative importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature importance from random forest\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "feature_names = data.columns[:-1].to_numpy()\n",
    "ranfor_perm_imp = permutation_importance(ranfor_model, X_test_std, y_test, n_repeats=10, random_state=42)\n",
    "ranfor_perm_sort_idx = ranfor_perm_imp.importances_mean.argsort()\n",
    "\n",
    "ranfor_tree_sort_idx = np.argsort(ranfor_model.feature_importances_)\n",
    "ranfor_indices = np.arange(0, len(ranfor_model.feature_importances_)) + 0.5\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "ax1.barh(ranfor_indices,ranfor_model.feature_importances_[ranfor_tree_sort_idx], height=0.7)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax1.set_xlabel(\"Importance Score\\n(b)\",fontsize=14)\n",
    "ax1.set_ylabel(\"Feature Name\",fontsize=14)\n",
    "ax1.set_yticklabels(feature_names[ranfor_tree_sort_idx])\n",
    "ax1.set_yticks(ranfor_indices)\n",
    "ax1.set_ylim((0, len(ranfor_model.feature_importances_)))\n",
    "ax2.boxplot(ranfor_perm_imp.importances[ranfor_perm_sort_idx].T,vert=False,labels=feature_names[ranfor_perm_sort_idx])\n",
    "ax2.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax2.set_xlabel(\"Importance Score\\n(b)\",fontsize=14)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"support_docs/Feature_Importance.pdf\",bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use cell below to make predictions with 6 models\n",
    "\n",
    "### List of input values in order (with codification)\n",
    " - Gender (Female=0,Male=1,Other=2)\n",
    " - Age (actual value)\n",
    " - Hypertension (No=0,Yes=1)\n",
    " - Heart Diserase (No=0,Yes=1)\n",
    " - Married (No=0,Yes=1)\n",
    " - Work Type (Private=0,Self-employed=1,children=2,Govt_job=3,Never_worked=4\")\n",
    " - Residence Type (Urban=0,Rural=1)\n",
    " - Blood Glucose Level (actual value)\n",
    " - BMI (actual value)\n",
    " - Smoking (never smoked=0,formerly smoked=1,smokes=2,unkown=3)\n",
    "\n",
    "### Output prediction value\n",
    " - Are you at risk of having a stroke? (No=0,Yes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singular Vector Machine (SVM)\n",
    "\n",
    "sample = [[1,76,1,1,0,0,0,150,32,1]]\n",
    "prediction = svm_model.predict(sample)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes (GNB)\n",
    "\n",
    "sample = [[1,76,1,1,0,0,0,150,32,1]]\n",
    "prediction = gnb_model.predict(sample)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression (LR)\n",
    "\n",
    "sample = [[1,76,1,1,0,0,0,150,32,1]]\n",
    "prediction = logit_model.predict(sample)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree (DT)\n",
    "\n",
    "sample = [[1,76,1,1,0,0,0,150,32,1]]\n",
    "prediction = dtree_model.predict(sample)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest (RF)\n",
    "\n",
    "sample = [[1,76,1,1,0,0,0,150,32,1]]\n",
    "prediction = ranfor_model.predict(sample)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Nearest Neighbors (KNN)\n",
    "\n",
    "sample = [[1,76,1,1,0,0,0,150,32,1]]\n",
    "prediction = knn_model.predict(sample)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In summary, two highest stroke prediction performance were achieved by decision tree and random forest; three most important features (in descending order) for stroke prediction were 'age', 'avg_glucose_level', and 'bmi'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
